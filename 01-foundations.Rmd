# 基础知识 {#prepare}

[本章概要]{.todo}

## 指数族 {#sec:exp}

Exponential family: 

$$
f(y;\theta,\phi) = \exp[(a(y) b(\theta) + c(\theta))/f(\phi) + d(y,\phi)]
$$

Poisson (with $\lambda \to \theta$, $x \to y$) ($\phi=1$):

\begin{equation*}
\begin{split}
f(y,\theta) & = \exp(-\theta) \theta^y/(y!) \\
            & = \exp\left( \underbrace{y}_{a(y)} 
                           \underbrace{\log \theta}_{b(\theta)} + 
                           \underbrace{(-\theta)}_{c(\theta)} + 
                           \underbrace{(- \log(y!))}_{d(y)} \right)
\end{split} (\#eq:another-exp-fam)
\end{equation*}

一般地，样本 $\mathbf{Y}$ 的分布服从指数族，即形如

\begin{equation}
f_{Y}(y;\theta,\phi) = \exp\big\{ \big(y\theta - b(\theta) \big)/a(\phi) + c(y,\phi) \big\}
(\#eq:common-exponential-family)
\end{equation}

其中，$a(\cdot),b(\cdot),c(\cdot)$ 是某些特定的函数。如果 $\phi$ 已知，这是一个含有典则参数 $\theta$ 的指数族模型，如果 $\phi$ 未知，它可能是含有两个参数的指数族。对于正态分布

\begin{equation}
\begin{aligned}
f_{Y}(y;\theta,\phi) & = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{(y - \mu)^2}{2\sigma^2}  \}  \\
 & = \exp\big \{ (y\mu - \mu^2/2)/\sigma^2 - \frac{1}{2}\big(y^2/\sigma^2 + \log(2\pi\sigma^2)\big) \big\}
\end{aligned} (\#eq:normal-distribution)
\end{equation}

通过与 \@ref(eq:common-exponential-family) 式对比，可知 $\theta = \mu$，$\phi = \sigma^2$，并且有

\[
a(\phi) = \phi, \quad b(\theta) = \theta^2/2, \quad c(y,\phi) = - \frac{1}{2}\{ y^2/\sigma^2 + \log(2\pi\sigma^2) \} 
\]

记 $l(\theta,\phi;y) = \log f_{Y}(y;\theta,\phi)$ 为给定样本点 $y$ 的情况下，关于 $\theta$ 和 $\phi$ 的对数似然函数。样本分布 $Y$ 的均值和方差具有如下关系

\begin{equation}
\mathsf{E}\big( \frac{\partial l}{\partial \theta} \big) = 0
(\#eq:mean-log-lik)
\end{equation}

和

\begin{equation}
\mathsf{E}\big( \frac{\partial^2 l}{\partial \theta^2} \big) + \mathsf{E}\big(\frac{\partial l}{\partial \theta}\big)^2  = 0
(\#eq:variance-log-lik)
\end{equation}

从 \@ref(eq:common-exponential-family) 式知

\[ l(\theta,\phi;y) = {y\theta - b(\theta)}/a(\phi) + c(y,\phi) \]

因此，

\begin{equation}
\begin{aligned}
\frac{\partial l}{\partial \theta} & = {y - b'(\theta)}/a(\phi)  \\
\frac{\partial^2 l}{\partial \theta^2}  & = - b''(\theta)/a(\phi)
\end{aligned} (\#eq:partial-log-lik)
\end{equation}

从 \@ref(eq:mean-log-lik) 式和 \@ref(eq:partial-log-lik)，可以得出

\[ 
0 = \mathsf{E}\big( \frac{\partial l}{\partial \theta} \big) = \big\{ \mu - b'(\theta) \big\}/a(\phi)
\]

所以

\[ \mathsf{E}(Y) = \mu = b'(\theta) \]

根据 \@ref(eq:variance-log-lik) 式和 \@ref(eq:partial-log-lik) 式，可得

\[ 0 = - \frac{b''(\theta)}{a(\phi)} + \frac{\mathsf{Var}(Y)}{a^2(\phi)} \]

所以

\[ \mathsf{Var}(Y) = b''(\theta)a(\phi) \]

可见，$Y$ 的方差是两个函数的乘积，一个是 $b''(\theta)$， 它仅仅依赖典则参数，被叫做方差函数，另一个是 $a(\phi)$，它独立于 $\theta$，仅仅依赖 $\phi$，方差函数可以看作是 $\mu$ 的函数，记作 $V(\mu)$。

函数 $a(\phi)$ 通常形如

\[ a(\phi) = \phi/w \]

其中 $\phi$ 可由 $\sigma^2$ 表示，故而也叫做发散参数 (dispersion parameter)，是一个与样本观察值相关的常数，$w$ 是已知的权重，随样本观察值变化。对正态分布模型而言，$w$ 的分量是 $m$ 个相互独立的样本观察值的均值，我们有

\[ a(\phi) = \sigma^2/m\]

所以，$w = m$。

根据 \@ref(eq:common-exponential-family)式，正态、泊松和二项分布的特征见表 \@ref(tab:common-characteristics)，其它常见分布见 Peter McCullagh 等 (1989年) [@McCullagh1989]。

Table: (\#tab:common-characteristics) 指数族内常见的一元分布的共同特征及符号表示^[(ref:footnote-tab-common-characteristics)] 

|                   |      正态分布      |      泊松分布      |      二项分布      |
| :---------------- | :----------------: | :----------------: | :----------------: | 
|  记号             | $N(\mu,\sigma^2)$  |       $P(\mu)$     |     $B(m,\pi)/m$   |
|  $y$ 取值范围     | $(-\infty,\infty)$ |     $0(1)\infty$   |  $\frac{0(1)m}{m}$ |
|  $\phi$           | $\phi = \sigma^2$  |         $1$        |        $1/m$       |
|  $b(\theta)$      | $\theta^2/2$       |  $\exp(\theta)$    |$\log(1+e^{\theta})$|
| $c(y;\theta)$     | $-\frac{1}{2}\big( \frac{y^2}{\phi} + \log(2\pi\phi) \big)$  |   $-\log(y!)$    | $\log\binom{m}{my}$ |   
| $\mu(\theta) = \mathsf{E}(Y;\theta)$  |  $\theta$   | $\exp(\theta)$ |  $e^{\theta}/(1+e^{\theta})$ |
| 联系函数：$\theta(\mu)$   |  identity |    log      |     logit      |
| 方差函数：$V(\mu)$        |   1       |   $\mu$     |  $\mu(1-\mu)$  |

(ref:footnote-tab-common-characteristics) 均值参数用 $\mu$ 表示，二项分布里用 $\pi$ 表示；典则参数用 $\theta$ 表示，定义见 \@ref(eq:common-exponential-family) 式，$\mu$ 和 $\theta$ 的关系在表 \@ref(tab:common-characteristics) 的第 6 和第 7 行给出。 

## 最小二乘估计 {#lse}

考虑如下线性模型的最小二乘估计

\begin{equation}
\mathsf{E}\mathbf{Y} = \mathbf{X}\boldsymbol{\beta}; \mathsf{Var}(\mathbf{Y}) = \sigma^2 \mathbf{I}_{n} (\#eq:linear-models)
\end{equation}

其中， $\mathbf{Y}$ 为 $n \times 1$ 维观测向量， $\mathbf{X}$ 为已知的 $n \times p (p \leq n)$ 阶设计矩阵，$\boldsymbol{\beta}$ 为 $p \times 1$ 维未知参数，$\sigma^2$ 未知，$\mathbf{I}_{n}$ 为 $n$ 阶单位阵。

```{definition, label="least-squares-estimate", name="最小二乘估计", echo=TRUE}
在模型 \@ref(eq:linear-models) 中，如果

\begin{equation}
(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^{\top}(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = \min_{\beta}(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^{\top}(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}) (\#eq:least-squares)
\end{equation}

\noindent 则称 $\hat{\boldsymbol{\beta}}$ 为 $\boldsymbol{\beta}$ 的最小二乘估计 (Least Squares Estimate，简称 LSE)。
```

```{theorem, label="unbiased", name="最小二乘估计", echo=TRUE}
若模型  \@ref(eq:linear-models) 中的 $\mathbf{X}$ 是列满秩的矩阵，则 $\boldsymbol{\beta}$ 的最小二乘估计为

\[
\hat{\boldsymbol{\beta}}_{LS} = ( \mathbf{X}^{\top}\mathbf{X} )^{-1}\mathbf{X}^{\top} \mathbf{Y}, \quad  \mathsf{Var}(\hat{\boldsymbol{\beta}}_{LS}) = \sigma^2 (\mathbf{X}^{\top}\mathbf{X})^{-1}  
\]

\noindent $\sigma^2$ 的最小二乘估计为

\[
\hat{\sigma^2}_{LS} = (\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}_{LS})^{\top}(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}_{LS})/(n - p)
\]

若将模型  \@ref(eq:linear-models) 的条件 $\mathsf{Var}(\mathbf{Y}) = \sigma^2 \mathbf{I}_{n}$ 改为 $\mathsf{Var}(\mathbf{Y}) = \sigma^2 \mathbf{G}$， $G(>0)$ 为已知正定阵，则$\boldsymbol{\beta}$ 的最小二乘估计为

\[
\tilde{\boldsymbol{\beta}}_{LS} = ( \mathbf{X}^{\top} G^{-1} \mathbf{X})^{-1} \mathbf{X}^{\top} G^{-1} \mathbf{Y} 
\]

\noindent 称 $\tilde{\boldsymbol{\beta}}_{LS}$ 为广义最小二乘估计 (Generalized Least Squares Estimate，简称 GLSE)，特别地，当 $G = \mathrm{diag}(\sigma^2_{1},\ldots,\sigma^2_{n})$，$\sigma^2_{i},i = 1,\ldots,n$ 已知时，称 $\tilde{\boldsymbol{\beta}}_{LS}$ 为加权最小二乘估计 (Weighted Least Squares Estimate，简称 WLSE)[@wang2004]
```


## 极大似然估计 {#def-mle}

```{definition, label="maximum-likelihood-estimate", name="极大似然估计", echo=TRUE}
设 $p(\mathbf{x};\boldsymbol{\theta}),\boldsymbol{\theta} \in \boldsymbol{\Theta}$ 是 $(\mathbb{R}^n,\mathscr{P}_{\mathbb{R}^n})$ 上的一族联合密度函数，对给定的 $\mathbf{x}$，称

\[ L(\boldsymbol{\theta};\mathbf{x}) = kp(\mathbf{x};\boldsymbol{\theta}) \]

\noindent 为 $\boldsymbol{\theta}$ 的似然函数，其中 $k > 0$ 是不依赖于 $\boldsymbol{\theta}$ 的量，常取 $k=1$。进一步，若存在 $(\mathbb{R}^n,\mathscr{P}_{\mathbb{R}^n})$ 到 $(\boldsymbol{\Theta},\mathscr{P}_{\boldsymbol{\Theta}})$ 的统计量 $\hat{\boldsymbol{\theta}}(\mathbf{x})$ 使

\[ L(\hat{\boldsymbol{\theta}}(\mathbf{x});\mathbf{x}) = \sup_{\boldsymbol{\theta}} L(\boldsymbol{\theta};\mathbf{x}) \]

\noindent 则 $\hat{\boldsymbol{\theta}}(\mathbf{x})$ 称为 $\boldsymbol{\theta}$ 的一个极大似然估计(Maximum Likelihood Eestimate，简称 MLE)。
```

概率密度函数很多可以写成具有指数函数的形式，如指数族，采用似然函数的对数通常更为简便。称

\[ l(\boldsymbol{\theta},\mathbf{x}) = \ln L(\boldsymbol{\theta},\mathbf{x}) \]

\noindent 为 $\boldsymbol{\theta}$ 的对数似然函数。对数变换是严格单调的，所以 $l(\boldsymbol{\theta},\mathbf{x})$ 与 $L(\boldsymbol{\theta},\mathbf{x})$ 的极大值是等价的。当 MLE 存在时，寻找 MLE 的常用方法是求导数。如果 $\hat{\boldsymbol{\theta}}(\mathbf{x})$ 是 $\boldsymbol{\Theta}$ 的内点，则 $\hat{\boldsymbol{\theta}}(\mathbf{x})$ 是下列似然方程组

\begin{equation}
\partial l(\boldsymbol{\theta},\mathbf{x})/ \partial \boldsymbol{\theta}_{i} = 0, \quad i = 1,\ldots, m (\#eq:likelihood-equations)
\end{equation}

\noindent 的解。$p(\mathbf{x};\boldsymbol{\theta})$ 属于指数族时，似然方程组 \@ref(eq:likelihood-equations) 的解唯一。

```{theorem, label="consistency", name="相合性", echo=TRUE}
设 $x_{1}, \ldots, x_{n}$ 是来自概率密度函数 $p(x;\theta)$ 的一个样本，叙述简单起见，考虑单参数情形，参数空间 $\boldsymbol{\Theta}$ 是一个开区间，$l(\theta;x) = \sum_{i=1}^{n}\ln p(x_{i};\theta)$。

若 $\ln (p;\theta)$ 在 $\boldsymbol{\Theta}$ 上可微，且 $p(x;\theta)$ 是可识别的（即 $\forall \theta_1 \neq \theta_2, \{x: p(x;\theta_1) \neq p(x; \theta_2)\}$ 不是零测集），则似然方程 \@ref(eq:likelihood-equations) 在 $n \to \infty$ 时，以概率 1 有解，且此解关于 $\theta$ 是相合的。
```

```{theorem, label="asymptotic-normality", name="渐近正态性", echo=TRUE}
假设 $\boldsymbol{\Theta}$ 为开区间，概率密度函数 $p(x;\theta), \theta \in \boldsymbol{\Theta}$ 满足

1. 在参数真值 $\theta_{0}$ 的邻域内，$\partial \ln p/\partial \theta, \partial^2 \ln p/\partial \theta^2, \partial^3 \ln p/\partial \theta^3$ 对所有的 $x$ 都存在；
2. 在参数真值 $\theta_{0}$ 的邻域内，$| \partial^3 \ln p/\partial \theta^3 | \leq H(x)$，且 $\mathsf{E}H(x) < \infty$；
3. 在参数真值 $\theta_{0}$ 处，

\begin{equation} 
\mathsf{E}_{\theta_{0}} \big[ \frac{ p'(x,\theta_{0}) }{ p(x,\theta_{0}) } \big] = 0, \quad
\mathsf{E}_{\theta_{0}} \big[ \frac{ p''(x,\theta_{0}) }{ p(x,\theta_{0}) } \big] = 0, \quad
I(\theta_{0}) = \mathsf{E}_{\theta_{0}} \big[ \frac{ p'(x,\theta_{0}) }{ p(x,\theta_{0}) } \big]^{2} > 0
\end{equation}

\noindent 其中撇号表示对 $\theta$ 的微分。记 $\hat{\theta}_{n}$ 为 $n \to \infty$ 时，似然方程组的相合解，则

\[ \sqrt{n}(\hat{\theta}_{n} - \theta_{0}) \longrightarrow  \mathcal{N}(\mathbf{0},I^{-1}(\theta))\]
```

## 随机过程的连续性和可微性

为记号简便起见，考虑一维空间下，随机过程 $S(x)$ 的均方连续性和可微性。

```{definition, label="continuous", name="连续性", echo=TRUE}
随机过程 $S(x)$ 满足

\[ \lim_{h \to 0} \mathsf{E}\big[ \{S(x + h) - S(x)\}^{2} \big] = 0 \] 

则称 $S(x)$ 是均方连续(mean-square continuous)的。
```

```{definition, label="differentiable", name="可微性", echo=TRUE}
随机过程 $S(x)$ 满足

\[ \lim_{h \to 0} \mathsf{E} \big[ \{ \frac{S(x+h) - S(x)}{h} - S'(x) \}^2 \big] = 0 \]

则称 $S(x)$ 是均方可微的，并且 $S'(x)$ 就是均方意义下的一阶导数。如果 $S'(x)$ 是均方可微的，则 $S(x)$ 是二次均方可微的，随机过程 $S(x)$ 的高阶均方可微性可类似定义。
```

Bartlett (1955 年) [@Bartlett1955] 得到如下结论

```{theorem, label="stationary-mean-square-properties", name="平稳随机过程的可微性", echo=TRUE}
自相关函数为 $\rho(u)$ 的平稳随机过程是 $k$ 次均方可微 (mean-square differentiable) 的，当且仅当 $\rho(u)$ 在 $u = 0$ 处是 $2k$ 次可微的。
```

## 平稳高斯过程 {#stationary-gaussian-process}

一般地，空间高斯过程 $\mathcal{S} = \{S(x),x\in\mathbb{R}^2\}$ 必须满足条件：任意给定一组空间位置 $x_1,x_2,\ldots,x_n, \forall x_{i} \in \mathbb{R}^2$， 每个位置上对应的随机变量 $S(x_i), i = 1,2,\ldots,n$ 的联合分布 $\mathcal{S} = \{S(x_1), S(x_2),\ldots,S(x_n)\}$ 是多元高斯分布，其由均值 $\mu(x) = \mathrm{E}[S(x)]$ 和协方差 $G_{ij} = \gamma(x_i,x_j) = \mathrm{Cov}\{S(x_i),S(x_j)\}$ 完全确定，即 $\mathcal{S} \sim \mathrm{MVN}(\mu_{S},G)$

平稳空间高斯过程需要空间高斯过程满足平稳性条件：其一， $\mu(x) = \mu, \forall x \in \mathbb{R}^2$， 其二，自协方差函数 $\gamma(x_i,x_j) = \gamma(u),u=\|x_{i} - x_{j}\|$。 可见均值 $\mu$ 是一个常数， 而自协方差函数 $\gamma(x_i,x_j)$ 只与空间距离有关。 注意到平稳高斯过程 $\mathcal{S}$ 的方差是一个常数，即 $\sigma^2 = \gamma(0)$， 然后可以定义自相关函数 $\rho(u) = \gamma(u)/\sigma^2$， 并且 $\rho(u)$ 满足对称性， $\rho(u) = \rho(-u)$， 因为对 $\forall u, \mathrm{Corr}\{S(x),S(x-u)\} = \mathrm{Corr}\{S(x-u), S(x)\} = \mathrm{Corr}\{S(x),S(x+u)\}$， 这里的第二个等式是根据平稳性得来的， 根据协方差的定义不难验证。 在本论文中如果不特别说明， 平稳就指上述协方差意义下的平稳， 这种平稳性条件广泛应用于空间统计数据建模。


## 修正的第三类贝塞尔函数 {#Modified-Bessel-functions}

空间过程的协方差函数是梅隆族时，需要用到修正的第三类贝塞尔函数 $\mathcal{K}_{\kappa}(u)$，它是修正的贝塞尔方程的解 [@Abramowitz1972]，函数形式如下

\begin{equation}
\begin{aligned}
I_{-\kappa}(u) & =  \sum_{m=0}^{\infty} \frac{1}{m!\Gamma(m + \kappa + 1)} \big(\frac{u}{2}\big)^{2m + \kappa} \\
\mathcal{K}_{\kappa}(u) & = \frac{\pi}{2} \frac{I_{-\kappa}(u) - I_{\kappa}(u)}{\sin (\kappa \pi)}
\end{aligned} (\#eq:besselK-function)
\end{equation}

其中 $u \geq 0$，$\kappa \in \mathbb{R}$，如果 $\kappa \in \mathbb{Z}$，则取该点的极限值，实际上 R 内置的函数 `besselK` 可以计算 $\mathcal{K}_{\kappa}(u)$ [@Campbell1980]

```{r Bessel-function,fig.cap="贝塞尔函数图像"}
knitr::include_graphics(path = "figures/bessel.png")
```


## 拉普拉斯近似 {#Laplace-approximation}

先回顾一下基本的泰勒展开，一个函数可以在点 $a$ 处展开成和的形式，有时候是无穷多项，可以使用其中的有限项最为近似，通常会使用前三项，即到达二阶导的位置。

\[
f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \ldots
\]

以基本的抛物线为例， $f(x) = x^2$，在 $a = 2$ 处展开

\[ f(x) = x^2, \quad f'(x) = 2x, \quad f''(x) = 2, \quad f'''(x) = 0 \]

因此，

\[ f(x) = x^2 = 2^2 + 2(2)(x-2) + \frac{2}{2}(x-2)^2 \]

拉普拉斯近似用正态分布来估计任意分布，它使用泰勒展开的前三项近似 $\log g(x)$，展开的位置是 $\hat{x}$，则 

\[
\log g(x) \approx \log g(\hat{x}) + \frac{\partial \log g(\hat{x})}{\partial x} (x - \hat{x}) + \frac{\partial^2 \log g(\hat{x})}{2\partial x^2} (x - \hat{x})^2
\]

在函数 $g(x)$ 的极值点 $\hat{x}$ 展开， $x = \hat{x}$ 一阶导是 0，用曲率去估计方差是 $\hat{\sigma}^2 = -1/\frac{\partial^2 \log g(\hat{x})}{2\partial x^2}$，再重写上述近似

\[ \log g(x) \approx \log g(\hat{x}) - \frac{1}{2\hat{\sigma}^2} (x - \hat{x})^2 \]

现在，用这个结果做正态近似，将上式两端取指数和积分，移去常数项

\[
\int g(x) \mathrm{d}x = \int \exp[\log g(x)] \mathrm{d}x \approx \mathrm{constant} \int \exp[- \frac{(x - \hat{x})^2}{2\hat{\sigma}^2}] \mathrm{d}x
\]

拉普拉斯方法用正态分布近似分布 $f(x)$， 其均值 $\hat(x)$，可以通过求解 $f'(x) = 0$ 获得，方差 $\hat{\sigma}^2 = -1/f''(\hat{x})$  

以卡方分布 $\chi^2$ 为例，

\begin{align*}
    f(x; k) & = \frac{ x^{k/2-1} \mathrm{e}^{-x/2} }{ 2^{k/2}\Gamma(k/2) }, x \geq 0 \\
  \log f(x) & = (k/2 - 1) \log x - x/2 \\
 \log f'(x) & = (k/2-1)/x - 1/2 = 0 \\
\log f''(x) & = -(k/2-1)/x^2
\end{align*}

所以

\[ \chi_{k}^2 \overset{LA}{\sim}  N(\hat{x} = k-2, \hat{\sigma}^2 = 2(k-2)) \]

自由度越大，近似效果越好，对于多元分布的情况不难推广，使用多元泰勒展开和黑塞矩阵即可表示。并且参数集 $\theta$ 有唯一的极大值点 $\hat{\theta}$ [@Tierney1986]

## Jeffreys 先验分布 {#Jeffreys-prior}

设 $\mathbf{x} = (x_1,\ldots,x_n)$ 是来自密度函数 $p(x|\theta)$ 的一个样本，其中 $\boldsymbol{\theta} = (\theta_1,\ldots,\theta_p)$ 是 $p$ 维参数向量。在对 $\boldsymbol{\theta}$ 无任何先验信息可用时， Jeffreys (1961年)利用变换群和 Harr 测度导出 $\boldsymbol{\theta}$ 的无信息先验分布可用 Fisher 信息阵的行列式的平方根表示。这种无信息先验分布常称为 Jeffreys 先验分布。其求取步骤如下：

1. 写出样本的对数似然函数 $l(\boldsymbol{\theta}|x) = \sum_{i=1}^{n}\ln p(x_i | \theta)$； 
2. 算出参数 $\boldsymbol{\theta}$ 的 Fisher 信息阵 
   $$\mathbf{I}(\boldsymbol{\theta}) = \mathsf{E}_{x|\theta} \big( - \frac{\partial^2 l}{\partial \theta_i \partial \theta_j} \big)_{i,j=1,\ldots,p}$$
   在单参数场合， $\mathbf{I}(\theta) = \mathsf{E}_{x|\theta} \big( - \frac{\partial^2 l}{\partial \theta^2} \big)$；
3. $\boldsymbol{\theta}$ 的无信息先验密度函数为 $\pi(\boldsymbol{\theta}) = [\det \mathbf{I}(\theta) ]^{1/2}$，在单参数场合， $\pi(\theta) = [\mathbf{I}(\theta) ]^{1/2}$



## 贝叶斯理论 {#bayes-methods}

[贝叶斯方法，贝叶斯定理，非信息先验分布，扁平先验]{.todo}

以标准线性模型为例介绍贝叶斯分析及其基本概念 [@Rasmussen2006]，为什么不用 RMSE 均方误差，WAIC pDIC 模型选择 loo K-CV  

贝叶斯定理 \@ref(fig:bayes-theorem)  贝叶斯定理 \@ref(eq:bayes-theorem)

```{r bayes-theorem,fig.cap="贝叶斯定理",echo=FALSE}
knitr::include_graphics(path = 'figures/bayes-theorem.png')
```

作为铺垫，先结合 SGLMM 模型介绍一下贝叶斯定理，其中，$\boldsymbol{\theta}$ 代表 SGLMM 模型中的参数，$\mathbf{Y}$ 是响应变量对应的观察值。

\begin{align}
\begin{array}{rcll}
p(\boldsymbol{\theta}|\mathbf{Y})  & =  & \displaystyle \frac{p(\boldsymbol{\theta},\mathbf{Y})}{p(\mathbf{Y})}
& \mbox{ [条件概率定义]}
\\[16pt]
& = & \displaystyle \frac{p(\mathbf{Y}|\boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathbf{Y})}
& \mbox{ [链式法则]}
\\[16pt]
& = & \displaystyle \frac{p(\mathbf{Y}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{\int_{\Theta}p(\mathbf{Y},\boldsymbol{\theta})d\boldsymbol{\theta}}
& \mbox{ [全概率公式]}
\\[16pt]
& = & \displaystyle \frac{p(\mathbf{Y}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{\int_{\Theta}p(\mathbf{Y}|\boldsymbol{\theta})p(\boldsymbol{\theta})d\boldsymbol{\theta}}
& \mbox{ [链式法则]}
\\[16pt]
& \propto & \displaystyle p(\mathbf{Y}|\boldsymbol{\theta})p(\boldsymbol{\theta})
& \mbox{ [$\mathbf{Y}$ 已知]}
\end{array} (\#eq:bayes-theorem)
\end{align}



## 贝叶斯数据分析 {#bayesian-data-analysis}

以一个广义线性模型为例说明贝叶斯数据分析的过程。模拟数据集 logit 来自 R包 **mcmc**，它包含5个变量，一个响应变量 y 和四个预测变量 x1，x2，x3，x4。频率派的分析可以用这样几行 R 代码实现

```{r frequentist-analysis,echo=TRUE}
library(mcmc)
data(logit)
fit <- glm(y ~ x1 + x2 + x3 + x4, data = logit, 
           family = binomial(), x = TRUE)
summary(fit)
```

现在，我们想用贝叶斯的方法来分析同一份数据，假定5个参数（回归系数）的先验分布是独立同正态分布，且均值为 0，标准差为 2。

该广义线性模型的对数后验密度（对数似然加上对数先验）可以通过下面的 R 命令给出

```{r echo=TRUE}
x <- fit$x
y <- fit$y
lupost <- function(beta, x, y) {
  eta <- as.numeric(x %*% beta)
  logp <- ifelse(eta < 0, eta - log1p(exp(eta)), -log1p(exp(-eta)))
  logq <- ifelse(eta < 0, -log1p(exp(eta)), -eta - log1p(exp(-eta)))
  logl <- sum(logp[ y == 1]) + sum(logq[y == 0])
  return(logl - sum(beta^2) / 8)
}
```

为了防止溢出 (overflow) 和巨量消失 (catastrophic cancellation)，计算 $\log(p)$ 和 $\log(q)$ 使用了如下技巧

\begin{align*}
p &= \frac{\exp(\eta)}{1 + \exp(\eta)} = \frac{1}{1 + \exp(- \eta)} \\
q &= \frac{1}{1 + \exp(\eta)} = \frac{\exp(- \eta)}{1 + \exp(- \eta)}
\end{align*}

然后对上式取对数

\begin{align*}
\log(p) &= \eta - \log(1 + \exp(\eta)) = - \log(1 + \exp(- \eta)) \\
\log(q) &= - \log(1 + exp(\eta)) = - \eta - \log(1 + \exp(-\eta))
\end{align*}

为防止溢出，我们总是让 exp 的参数取负数，也防止在 $|\eta|$ 很大时巨量消失。比如，当 $\eta$ 为很大的正数时，

\begin{align*}
p & \approx  1  \\
q & \approx  0 \\
\log(p) & \approx  - \exp(-\eta) \\
\log(q) & \approx  - \eta - \exp(-\eta)
\end{align*}

当 $\eta$ 为很小的数时，使用 R 内置的函数 log1p 计算，当 $\eta$ 为大的负数时，情况类似^[更加精确的计算 $\log(1-\exp(-|a|)), |a| \ll 1$ 可以借助 **Rmpfr** 包 <https://r-forge.r-project.org/projects/rmpfr/>]。

有了上面这些准备，现在可以运行随机游走 Metropolis 算法模拟后验分布

```{r echo=TRUE}
set.seed(2018)
beta.init <- as.numeric(coefficients(fit))
fit.bayes <- metrop(obj = lupost, initial = beta.init, 
                    nbatch = 1e3, blen = 1, nspac = 1, x = x, y = y)
names(fit.bayes)
fit.bayes$accept
```

这里使用的 metrop 函数的参数说明如下：

- 自编的 R 函数 lupost 计算未归一化的 Markov 链的平稳分布（后验分布）的对数密度；
- beta.init 表示 Markov 链的初始状态；
- Markov 链的 batches；
- x,y 是提供给目标函数 lupost 的额外参数




## 维数灾难与蒙特卡罗积分 {#Curse-of-Dimensionality}

一般地，混合效应模型的统计推断总是不可避免的要面对高维积分，处理高维积分的方法一个是寻找近似方法避免求积分，一个是寻找有效的随机模拟方法直接求积分。这里，介绍蒙特卡罗方法求积分，以计算 $N$ 维超立方体的内切球的体积为例说明。

假设我们有一个 $N$ 维超立方体，其中心在坐标 $\mathbf{0} = (0,\ldots,0)$。超立方体在点 $(\pm 1/2,\ldots,\pm 1/2)$，有 $2^{N}$ 个角落，超立方体边长是1，$1^{N}=1$，所以它的体积是1。

如果 $N=1$，超立方体是一条从 $-\frac{1}{2}$ 到 $\frac{1}{2}$ 的单位长度的线，如果 $N=2$，超立方体是一个单位正方形，对角是 $\left( -\frac{1}{2}, -\frac{1}{2} \right)$ 和 $\left( \frac{1}{2}, \frac{1}{2} \right)$，如果 $N=3$，超立方体就是单位体积的立方体，对角是 $\left( -\frac{1}{2}, -\frac{1}{2}, -\frac{1}{2} \right)$ 和 $\left( \frac{1}{2}, \frac{1}{2}, \frac{1}{2} \right)$，依此类推，$N$ 维超立方体体积是1，对角是 $\left( -\frac{1}{2}, \ldots, -\frac{1}{2} \right)$ 和 $\left( \frac{1}{2}, \ldots, \frac{1}{2} \right)$

现在，考虑 $N$ 维超立方体的内切球，我们把它称为 $N$ 维超球，它的中心在原点，半径是 $\frac{1}{2}$。我们说点 $y$ 在超球内，意味着它到原点的距离小于半径，即 $\| y \| < \frac{1}{2}$。

一维情形下，超球是从的线，包含了整个超立方体。二维情形下，超球是中心在原点，半径为 $\frac{1}{2}$ 的圆。三维情形下，超球是立方体的内切球。

我们知道单位超立方体的体积是1，但是其内的内切球的体积是多少呢？我们已经学过如何去定义一个积分计算半径为 $r$ 的二维球（即圆）的体积（即面积）是 $\pi r^2$，三维情形下，内切球是 $\frac{4}{3}\pi r^3$。但是更高维的欧式空间里，内切球的体积是多少呢？

我们当然可以去计算越来越复杂的多重积分，但是这里我们介绍采样的方法去计算积分，即所谓的蒙特卡罗方法，由乌拉姆 (S. Ulam)、冯$\cdot$诺依曼(J. von Neumann) 和梅特罗波利斯 (N. Metropolis) 等 在美国核武器研究实验室创立，当时正值二战期间，为了研制原子弹，出于保密的需要，与随机模拟相关的技术就代号蒙特卡罗。现在，蒙特卡罗方法占据现代统计计算的核心地位，特别是与贝叶斯相关的领域。

用蒙特卡罗方法去计算单位超立方体内的超球，首先我们需要在单位超立方体内产生随机点，然后计算落在超球内点的比例，即超球的体积。随着点的数目增加，估计的体积会收敛到真实的体积。因为这些点都独立同均匀分布，根据中心极限定理，误差下降的比率是 $\mathcal{O}\left( 1 / \sqrt{n} \right)$，这也意味着每增加一个小数点的准确度，样本量要增加 100 倍。

表 \@ref(tab:calculate-volume-of-hyperball) 列出了前10维超球的体积，随着维数的增加，超球的体积迅速变小，超立方体内随机点的个数是 100000。这里有一个反直观的现象，内切球的体积竟然随着维数的增加变小，并且在10维的情形下，内切球的体积已不到超立方体的 0.3\%。

Table: (\#tab:calculate-volume-of-hyperball) 前10维单位超立方体内切球的体积（已经四舍五入保留小数点后三位）

| 维数 |   1     |   2     |    3     |    4     |   5     |   6     |    7     |    8     |    9    |    10   |
| :--- | :-----: | :-----: | :------: | :------: | :-----: | :-----: | :------: | :------: | :-----: | :-----: |
| 体积 | 1.000   | 0.784   | 0.525    | 0.307    | 0.166   | 0.081   |  0.037   |  0.016   | 0.006   | 0.0027  |



## 采样器与 Stan {#Samplers}

随机模拟的基础是有高质量的伪随机数，如何生成和检验伪随机数的质量参见黄湘云的文章 [@Huang2017COS]。通过随机模拟的方式从总体中获取样本，需要一个抽样（也叫采样）的过程，不同的采样算法（也叫采样器）在适用范围和采样效率方面有不同。在贝叶斯计算中，常用的采样器有 Gibbs， Metropolis 和汉密尔顿蒙特卡罗 (Hamiltonian Monte Carlo，简称 HMC) 三类。 

Matthew D. Hoffman 和 Andrew Gelman (2014年) [@hoffman2014] 提出的 No-U-Turn 采样器属于 HMC 衍生的采样器。

Stan 是一门基于 C++ 的高级编程语言，用户只需提供数据、模型和参数初值，目标后验分布的 Markov 链的模拟过程是自动实现的。除了可以完全在 Stan 脚本中写模型外，Stan 还提供其他编程语言的接口，如 R，Python 和 MATLAB 等，使得熟悉其他编程语言的用户也可以比较方便地调用。 

以分层正态模型为例，数据集 8schools 来自 Andrew Gelman 和 John B. Carlin 等 (2003年) [@Gelman2003]，由美国教育考试服务调查搜集，用以分析不同的辅导项目对学生考试分数的影响，调查结果用来帮助高校招生。分别随机调查了 8 所高中，输出变量是一个分数，培训效应的估计 $y_j$，其样本方差 $\sigma^2_j$，数据集见表 \@ref(tab:eight-high-schools)。

Table: (\#tab:eight-high-schools) 8 schools 数据集

|   School   |   A   |   B   |   C   |   D   |   E   |   F   |   G   |   H   |
|:----------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|   $y_i$    |  28   |   8   |   -3  |   7   |   -1  |   1   |   18  |   12  |
| $\sigma_i$ |  15   |  10   |   16  |   11  |    9  |   11  |   10  |   18  |

分层模型可以在 Stan 中写成如下形式，在工作目录下把它保存为 `8schools.stan ` 

```{r code-8schools,comment=NA}
cat(readLines("code/8schools.stan"),sep = "\n")
```

上述代码块的第一段提供数据：学校的数目 $J$；估计值 $y_1,\ldots,y_{J}$；标准差 $\sigma_1,\ldots,\sigma_{J}$，数据类型可以是整数、实数，结构可以是向量，或更一般的数组，还可以带约束，如在这个模型中 $J$ 限制为非负， $\sigma_{J}$ 必须是正的，另外两个反斜杠 // 表示注释。

第二段代码声明参数：模型中的待估参数，学校总体的效应 $\theta_j$，均值 $\mu$，标准差 $\tau$，学校水平上的误差 $\eta$ 和效应 $\theta$。在这个模型中，用 $\mu,\tau,\eta$ 表示 $\theta$ 而不是直接声明 $\theta$ 作一个参数，通过这种参数化，采样器的运行效率会提高，还应该尽量使用向量化操作代替 for 循环语句。

最后一段是模型：稍微注意的是，正态分布的符号 $N(\cdot,\cdot)$ 中后一个位置是方差，而 Stan 代码中使用的是标准差。`target += normal_lpdf(y | theta, sigma);`  和 `y ~ normal(theta, sigma);`
对模型的贡献是一样的，都使用正态分布的对数概率密度函数，只是后者扔掉了对数密度函数的常数项而已，这对于 Stan 的采样、近似或优化算法没有影响。

算法运行的硬件环境是 16 核 32 线程主频 2.8 GHz 英特尔至强 E5-2680 处理器

```{r run-8schools,echo=TRUE,eval=FALSE}
# 安装依赖
lapply(c(
  "ggplot2", "StanHeaders", "rstan"
), function(pkg) {
  if (system.file(package = pkg) == "") install.packages(pkg)
})
# 加载依赖
library(ggplot2)
library(StanHeaders)
library(rstan)
# 设置环境
options(mc.cores = ceiling(parallel::detectCores()/2))
rstan_options(auto_write = TRUE)
# 提供数据
schools_dat <- list(
  J = 8,
  y = c(28, 8, -3, 7, -1, 1, 18, 12),
  sigma = c(15, 10, 16, 11, 9, 11, 10, 18)
)
# 拟合模型
fit <- stan(
  model_name = "8schools",
  model_code = readLines("code/8schools.stan"),
  data = schools_dat,
  iter = 10000, chains = 4,
  seed = 2018
)
```


<https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started>


